{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.47, 0.6, 0.67, 0.89, 0.98\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "openai.api_key = \"\"\n",
    "\n",
    "\n",
    "history = []\n",
    "token = 0\n",
    "token_limit = 16384\n",
    "\n",
    "\n",
    "def OpenaiHandshake(msg, system, model=\"gpt-3.5-turbo-16k-0613\"):\n",
    "    history = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": msg\n",
    "    }]\n",
    "    response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=system+history,\n",
    "            temperature=1,\n",
    "            presence_penalty=1\n",
    "        )\n",
    "    response_m = response.choices[0].message\n",
    "        \n",
    "    return response_m, response_m.content\n",
    "\n",
    "def TokenCheck(content):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = encoding.encode(content)\n",
    "    token_count = len(tokens)\n",
    "    return token_count\n",
    "\n",
    "def TokenReflection(history):\n",
    "    global token\n",
    "    token_ = 0\n",
    "    for item in history:\n",
    "        msg = item[\"content\"]\n",
    "        token_ += TokenCheck(msg)\n",
    "    token = token_\n",
    "    print(\"[Token: {token}]\".format(token=token))\n",
    "    return token_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleData = \"\"\"\n",
    "The user want data for neural network testing purposes. You are supposed to create dataset in JSON format for me. The paragraphs should each be about 80 words. \n",
    "\n",
    "The following is an example of a dataset.\n",
    "\n",
    "Query: evolution of moral standards.\n",
    "Example dataset:\n",
    "[\n",
    "    {\n",
    "        \"id\": \"0\",\n",
    "        \"relevance\": \"high\",\n",
    "        \"queryTerm\": true,\n",
    "        \"queryFreq\": 5,\n",
    "        \"topic\": \"Philosophy\",\n",
    "        \"style\": \"formal\",\n",
    "        \"type\": \"introduction\",\n",
    "        \"entities\": [\n",
    "            \"Aristotle\",\n",
    "            \"Plato\",\n",
    "            \"Socrates\",\n",
    "            \"virtue ethics\"\n",
    "        ],\n",
    "        \"correctness\": true,\n",
    "        \"text\": \"The origins of moral philosophy in the Western tradition trace back to ancient Greek philosophers like Socrates, Plato and Aristotle. In his Nicomachean Ethics, Aristotle systematically formulated a theory of virtue ethics that described the path to human flourishing through cultivating virtues of character. \"\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def SystemPrompt(example):\n",
    "    systemPrompt = \"\"\"\n",
    "    You are a data science specilized robot that only provides dataset in JSON format and nothing else (no conversations nor explainations other than the dataset in JSON format that has been requested by the user). Each dataset should be different to others. There should be 4 types of relevance: high, medium, low, and none.\n",
    "    \"\"\"\n",
    "    systemPrompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": systemPrompt\n",
    "        }\n",
    "    ]\n",
    "    if example:\n",
    "        systemPrompt.append( {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": exampleData\n",
    "        } )\n",
    "    \n",
    "    return systemPrompt\n",
    "\n",
    "def Continue(start, batch, query):\n",
    "    id1 = start\n",
    "    id2 = id1 + batch\n",
    "    msg = 'Starting id={id1}. Ending by id={id2}. Provide me {no} datasets in JSON list. Query is \"{query}\". 80 words for each paragraph.'\n",
    "    msg = msg.format(\n",
    "        id1=id1,\n",
    "        id2=id2,\n",
    "        no=batch,\n",
    "        query=query\n",
    "    )\n",
    "    return msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting id=0. Ending by id=30. Provide me 30 datasets in JSON list. Query is \"neurobiology of altruism\". 80 words for each paragraph.\n"
     ]
    }
   ],
   "source": [
    "query = \"neurobiology of altruism\"\n",
    "batch = 30\n",
    "id = 0\n",
    "target = 150\n",
    "\n",
    "dataBank = []\n",
    "\n",
    "while id < target:\n",
    "    msg = Continue(id, batch, query)\n",
    "    print(msg)\n",
    "    response, data = OpenaiHandshake(\n",
    "        msg,\n",
    "        SystemPrompt(True),\n",
    "        \"gpt-3.5-turbo-16k-0613\"\n",
    "    )\n",
    "    dataBank.append(data)\n",
    "    print(data)\n",
    "    print(\"\\n\\n\")\n",
    "    id += batch\n",
    "\n",
    "print(\"Finish\")\n",
    "\n",
    "# 6m 18s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataBank_JSON = []\n",
    "brokenBlock = []\n",
    "id = 0\n",
    "for block in dataBank:\n",
    "    try:\n",
    "        block_ = json.loads(block)\n",
    "        for item in block_:\n",
    "            item_ = item\n",
    "            item_[\"id\"] = id\n",
    "            dataBank_JSON.append(item_)\n",
    "            id += 1\n",
    "    except:\n",
    "        brokenBlock.append(block)\n",
    "        print(\"Found broken block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = []\n",
    "medium = []\n",
    "low = []\n",
    "none = []\n",
    "for item in dataBank_JSON:\n",
    "    if item[\"relevance\"] == \"high\":\n",
    "        high.append(item[\"id\"])\n",
    "    elif item[\"relevance\"] == \"medium\":\n",
    "        medium.append(item[\"id\"])\n",
    "    elif item[\"relevance\"] == \"low\":\n",
    "        low.append(item[\"id\"])\n",
    "    elif item[\"relevance\"] == \"none\":\n",
    "        none.append(item[\"id\"])\n",
    "\n",
    "metaData = {\n",
    "    \"author\": {\n",
    "        \"name\": \"Jason Li\",\n",
    "        \"email\": \"pakaLFZ@gmail.com\",\n",
    "        \"organizationEmail\": \"fl822@ic.ac.uk\"\n",
    "        },\n",
    "    \"overview\": {\n",
    "        \"topic\": \"evolution of moral standards\",\n",
    "        \"numParagraphs\": len(dataBank_JSON),\n",
    "        \"example_ranking\": {\n",
    "            \"ranking_types\": [\n",
    "                \"high\",\n",
    "                \"medium\",\n",
    "                \"low\",\n",
    "                \"none\"\n",
    "            ],\n",
    "            \"high\": high,\n",
    "            \"medium\": medium,\n",
    "            \"low\": low,\n",
    "            \"none\": none\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"query\": query,\n",
    "    \"data\": dataBank_JSON\n",
    "}\n",
    "\n",
    "datasetName = \"dataset_{query}.json\".format(query=query.replace(\" \", \"-\"))\n",
    "log = open(datasetName, \"w\", encoding=\"utf8\")\n",
    "log.write(json.dumps(metaData, indent=4))\n",
    "log.flush()\n",
    "print(\"databank stored\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
