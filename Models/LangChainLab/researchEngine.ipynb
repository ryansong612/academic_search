{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Read more\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/chroma_self_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Environment does not have key: OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY=\"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\"\n",
    "SERPAPI_API_KEY = \"a2995783e8b1b09d262bc6ee57f8d1d22873bd68146f700b273c51e50642a9b0\"\n",
    "%env OPENAI_API_KEY\n",
    "%env SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "source = \"../../DataBank/dataset_doc2vec.json\"\n",
    "source = json.loads(open(source, \"r\", encoding=\"utf8\").read())\n",
    "\n",
    "query = source[\"query\"]\n",
    "\n",
    "paragraphs = source[\"data\"][:20]\n",
    "paragraphs = [paragraph['text'] for paragraph in paragraphs]\n",
    "\n",
    "relevance_truth = source[\"data\"][:20]\n",
    "relevance_truth = [item['relevance'] for item in relevance_truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for item in source[\"data\"][:20]:\n",
    "    relevance = item[\"relevance\"]\n",
    "    doc_ = Document(\n",
    "        page_content=item[\"text\"],\n",
    "        metadata={\n",
    "            \"relevance\": relevance\n",
    "        }\n",
    "    )\n",
    "    doc.append(doc_)\n",
    "vectorstore = Chroma.from_documents(doc, embeddings)\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"relevance\",\n",
    "        description=\"this makes no sense\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Doc2vec has been applied to diverse document modeling tasks including sentiment analysis, document classification, search and information retrieval, clustering documents by similarity, and assessing semantic similarity - typically using the cosine similarity of resulting paragraph vectors as features within supervised downstream models.', metadata={'relevance': 'high'}),\n",
       " Document(page_content='With its strong performance across applications in areas like document classification, clustering, search and information retrieval, sentiment analysis and semantic similarity, doc2vec provides an efficient and flexible methodology for unsupervised learning of paragraph vectors capturing semantic meaning from texts of arbitrary length.', metadata={'relevance': 'high'}),\n",
       " Document(page_content='With its state-of-the-art results and simplicity of generating semantic document embeddings, doc2vec has proven useful across a wide range of document modeling tasks including sentiment analysis, search/information retrieval, document classification, clustering, and assessing semantic similarity between texts of arbitrary length.', metadata={'relevance': 'high'}),\n",
       " Document(page_content='With competitive results on benchmarks, doc2vec provides an efficient approach to generating high-quality distributed paragraph vectors capturing semantic meaning from entire documents, useful across applications like document classification, clustering, search and information retrieval.', metadata={'relevance': 'high'}),\n",
       " Document(page_content='The doc2vec framework extends word2vec to allow unsupervised learning of distributed paragraph vector representations that have proven useful for semantic modeling of texts.', metadata={'relevance': 'low'}),\n",
       " Document(page_content='The doc2vec framework was proposed as an unsupervised learning methodology to generate fixed-length distributed vector representations that encode semantic meaning for text documents of arbitrary length. As an extension of word2vec word embedding techniques, doc2vec offers two main architectures - distributed bag of words (dbow) which ignores word order but uses document ID markers, and distributed memory model of paragraph vectors (dmpv) which concatenates document and word vectors to retain sequential context.', metadata={'relevance': 'high'}),\n",
       " Document(page_content='As an extension of the seminal word2vec technique for learning distributed vector representations of words, doc2vec enables unsupervised learning of paragraph vectors to capture semantics for entire documents of arbitrary length.', metadata={'relevance': 'low'}),\n",
       " Document(page_content='Doc2vec extends the word2vec distributed representation methodology to allow unsupervised learning of paragraph vectors capturing semantic meaning from texts of arbitrary length, enabling effective document-level modeling.', metadata={'relevance': 'low'}),\n",
       " Document(page_content='As an extension of the word2vec framework for learning high-quality word embeddings, doc2vec provides an efficient and flexible methodology for generating distributed paragraph vector representations capturing semantic meanings of documents.', metadata={'relevance': 'low'}),\n",
       " Document(page_content='Doc2vec extends the word2vec framework to unsupervised learning of fixed-length distributed paragraph vector representations that capture semantic meaning from entire documents.', metadata={'relevance': 'low'}),\n",
       " Document(page_content='By approximating the conditional log likelihood using negative sampling rather than computing a expensive normalized softmax, doc2vec enables efficient training of high-quality distributed paragraph vectors useful for downstream document analysis tasks.', metadata={'relevance': 'medium'}),\n",
       " Document(page_content='Both the dbow and dmpv architectures in doc2vec apply noise contrastive estimation and negative sampling techniques to accelerate training by avoiding expensive normalized softmax computation over large vocabularies during the language modeling objective.', metadata={'relevance': 'medium'}),\n",
       " Document(page_content='The distributed bag of words and distributed memory paragraph vector architectures in doc2vec make different tradeoffs - dbow ignores word order but is faster to train, while dmpv preserves order through vector concatenation but has more parameters.', metadata={'relevance': 'medium'}),\n",
       " Document(page_content='To improve computational efficiency during training, doc2vec approaches make use of negative sampling and noise contrastive estimation (NCE) loss to avoid expensive hierarchical softmax calculations over the entire vocabulary typically required in language modeling objectives.', metadata={'relevance': 'medium'}),\n",
       " Document(page_content='Negative sampling provides an approximation to softmax for training the language modeling objective efficiently in doc2vec, avoiding expensive normalization over large vocabularies by only updating a small random sample of output weights.', metadata={'relevance': 'medium'}),\n",
       " Document(page_content=\"The monumental discovery of the long-theorized Higgs boson at the Large Hadron Collider in 2012 confirmed experimental detection of a particle critical to the Standard Model's mechanism for explaining mass through electroweak symmetry breaking.\", metadata={'relevance': 'none'}),\n",
       " Document(page_content='Well-designed randomized controlled trials are considered the gold standard in evidence-based medicine for minimizing bias when evaluating the safety and efficacy of clinical interventions.', metadata={'relevance': 'none'}),\n",
       " Document(page_content=\"Einstein's revolutionary theory of general relativity fundamentally reshaped understandings of gravitation, space and time by describing gravity as geometric curvature of spacetime, providing an improved scientific model of cosmology confirmed through observations like the precession of Mercury's orbit and gravitational lensing.\", metadata={'relevance': 'none'}),\n",
       " Document(page_content='The theory of cosmic inflation posits an exponentially fast expansion of space by at least a factor of 10^78 in an infinitesimal fraction of a second after the Big Bang, likely driven by a negative-pressure vacuum energy density and generating primordial quantum fluctuations that became the seeds for later galaxy formation.', metadata={'relevance': 'none'}),\n",
       " Document(page_content='Analysis of tiny temperature fluctuations in the cosmic microwave background radiation leftover from the Big Bang some 380,000 years afterward provides insights into conditions in the extremely early universe starting from about 10^-36 seconds.', metadata={'relevance': 'none'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, search_kwargs={\"k\": 20}\n",
    ")\n",
    "retriever.get_relevant_documents(\"mechanisms and applications of doc2vec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
