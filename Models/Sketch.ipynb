{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "    W1 = tf.Variable(initializer(shape = (10, 512) ))\n",
    "    b1 = tf.Variable(initializer(shape = (10,1)))\n",
    "    W2 = tf.Variable(initializer(shape = (4, 10) ))\n",
    "    b2 = tf.Variable(initializer(shape = (4,1)))\n",
    "    \n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1, \"W2\" : W2, \"b2\" : b2}\n",
    "    return parameters\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    Z1 = tf.math.add(tf.linalg.matmul(W1, X), b1)\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.math.add(tf.linalg.matmul(W2, A1), b2)\n",
    "    \n",
    "    \n",
    "    return Z2    \n",
    "\n",
    "def compute_total_loss(logits, labels):\n",
    "    total_loss = tf.reduce_sum(tf.keras.losses.categorical_crossentropy(tf.transpose(labels),tf.transpose(logits),from_logits = True))\n",
    "    return total_loss\n",
    "\n",
    "def model(x_train_norm, y_train_label, x_test_norm, y_test_label, learning_rate = 0.1, num_epochs = 1500, print_cost = True):\n",
    "    costs = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    m = x_train_norm.shape[1]\n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_total_loss = 0.\n",
    "        \n",
    "        #We need to reset object to start measuring from 0 the accuracy each epoch\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1. predict\n",
    "            Z2 = forward_propagation(x_train_norm, parameters)\n",
    "\n",
    "            # 2. loss\n",
    "            minibatch_total_loss = compute_total_loss(Z2, y_train_label)  \n",
    "        train_accuracy.update_state(y_train_label, Z2)\n",
    "\n",
    "        trainable_variables = [W1, b1, W2, b2]\n",
    "        grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        epoch_total_loss += minibatch_total_loss\n",
    "        \n",
    "        # We divide the epoch total loss over the number of samples\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_total_loss))\n",
    "            print(\"Train accuracy:\", train_accuracy.result())\n",
    "            \n",
    "            # We evaluate the test set every 10 epochs to avoid computational overhead\n",
    "            Z2 = forward_propagation(x_test_norm, parameters)\n",
    "            test_accuracy.update_state(y_test_label, Z2)\n",
    "            print(\"Test_accuracy:\", test_accuracy.result())\n",
    "\n",
    "            costs.append(epoch_total_loss)\n",
    "            train_acc.append(train_accuracy.result())\n",
    "            test_acc.append(test_accuracy.result())\n",
    "            test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc\n",
    "    \n",
    "    \n",
    "parameters, costs, train_acc, test_acc = model(x_train_norm,y_train_label, x_test_norm, y_test_label, num_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
